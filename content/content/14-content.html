---
title: "Ethics and open science"
linktitle: "14: Ethics and open science"
date: "2020-11-30"
start_date: "2020-11-30"
end_date: "2020-12-04"
menu:
  content:
    parent: Course content
    weight: 14
type: docs
bibliography: "../../static/bib/references.bib"
csl: "../../static/bib/chicago-syllabus-no-bib.csl"
# slides: "14-slides"
output:
  blogdown::html_page:
    toc: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>

<div id="TOC">
<ul>
<li><a href="#readings">Readings</a></li>
<li><a href="#slides">Slides</a></li>
<li><a href="#videos">Videos</a></li>
</ul>
</div>

<div id="readings" class="section level2">
<h2>Readings</h2>
<p>This looks like a lot, but most of these are quite short.</p>
<p>Keep in mind throughout all these readings that an “algorithm” in these contexts is typically some fancy type of regression model where the outcome variable is something binary like “Safe babysitter/unsafe babysitter,” “Gave up seat in past/didn’t give up seat in past”, or “Violated probation in past/didn’t violate probation in past”, and the explanatory variables are hundreds of pieces of data that might predict those outcomes (social media history, flight history, race, etc.).</p>
<p>Data scientists build a (sometimes proprietary and complex) model based on existing data, plug in values for any given new person, multiply that person’s values by the coefficients in the model, and get a final score in the end for how likely someone is to be a safe babysitter or how likely someone is to return to jail.</p>
<ul>
<li><i class="far fa-file-pdf"></i> Miguel A. Hernán, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5888052/">“The C-Word: Scientific Euphemisms Do Not Improve Causal Inference From Observational Data”</a> <span class="citation">Miguel A. Hernán, “The c-Word: Scientific Euphemisms Do Not Improve Causal Inference from Observational Data,” <em>American Journal of Publich Health</em> 108, no. 5 (May 2018): 616–19, doi:<a href="https://doi.org/10.2105/AJPH.2018.304337" role="doc-biblioref">10.2105/AJPH.2018.304337</a>.</span></li>
<li><i class="fas fa-external-link-square-alt"></i> Hannah Fresques and Meg Marco, <a href="https://www.propublica.org/nerds/hadley-wickham-your-default-position-should-be-skepticism-and-other-advice-for-data-journalists">“‘Your Default Position Should Be Skepticism’ and Other Advice for Data Journalists From Hadley Wickham,”</a> <em>ProPublica</em>, June 10, 2019</li>
<li><i class="fas fa-external-link-square-alt"></i> DJ Patil, <a href="https://medium.com/@dpatil/a-code-of-ethics-for-data-science-cda27d1fac1">“A Code of Ethics for Data Science”</a>
<ul>
<li><i class="fas fa-book"></i> Mike Loukides, Hilary Mason, and DJ Patil, <a href="https://www.amazon.com/dp/B07GTC8ZN7"><em>Ethics and Data Science</em></a><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li>
</ul></li>
<li><i class="fas fa-external-link-square-alt"></i> <a href="https://medium.com/@AINowInstitute/ai-in-2018-a-year-in-review-8b161ead2b4e">“AI in 2018: A Year in Review”</a></li>
<li><i class="fas fa-external-link-square-alt"></i> <a href="https://www.nytimes.com/2018/05/04/books/review/automating-inequality-virginia-eubanks.html">“How Big Data Is ‘Automating Inequality’”</a></li>
<li><i class="fas fa-external-link-square-alt"></i> <a href="https://annenberg.usc.edu/news/diversity-and-inclusion/algorithms-oppression-safiya-noble-finds-old-stereotypes-persist-new">“In ‘Algorithms of Oppression,’ Safiya Noble finds old stereotypes persist in new media”</a></li>
<li><i class="fas fa-podcast"></i> <a href="https://99percentinvisible.org/episode/the-age-of-the-algorithm/">99% Invisible, “The Age of the Algorithm”</a>: Note that this is a podcast, or a 20ish minute audio story. <strong>Listen to this.</strong> The rest of the things on this page are helpful and supplementary (very few podcasts provide this much extra information), but you don’t need to go through it all.</li>
<li><i class="fas fa-podcast"></i> <a href="https://www.wnycstudios.org/podcasts/otm/segments/biased-algorithms-biased-world">On the Media, “Biased Algorithms, Biased World”</a></li>
<li><i class="fas fa-external-link-square-alt"></i> <a href="https://www.washingtonpost.com/technology/2018/11/16/wanted-perfect-babysitter-must-pass-ai-scan-respect-attitude/?utm_term=.080010494ed5">“Wanted: The ‘perfect babysitter.’ Must pass AI scan for respect and attitude.”</a></li>
<li><i class="fas fa-external-link-square-alt"></i> <a href="https://qz.com/1427621/companies-are-on-the-hook-if-their-hiring-algorithms-are-biased/">“Companies are on the hook if their hiring algorithms are biased”</a></li>
<li><i class="fas fa-external-link-square-alt"></i> <a href="https://www.popsci.com/recidivism-algorithm-random-bias">“Courts use algorithms to help determine sentencing, but random people get the same results”</a></li>
<li><i class="fab fa-twitter-square"></i> <a href="https://twitter.com/dhh/status/1192540900393705474">David Heinemeier Hansson’s rant on the Apple Card</a>
<ul>
<li>And <a href="https://dhh.dk/2019/about-the-apple-card.html">Jamie Heinemeier Hansson’s response</a></li>
</ul></li>
</ul>
</div>
<div id="slides" class="section level2">
<h2>Slides</h2>
<p>Slides go here.</p>
</div>
<div id="videos" class="section level2">
<h2>Videos</h2>
<p>Videos go here.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This concise booklet is the result of DJ Patil’s call for ethics in the previous post.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
